---
layout: post
title: Bagging
comments: True
---

Bagging is one of the core principles of random forests. In bagging several models are trained and for each model only a subset of the training observations is used to train each model, which are also called bagged models. 
The subsets can be drawn in different ways:

* For each model draw randomly observations with replacement
* For each model draw randomly observations without replacement
* Divide the training dataset into several subsets and train each model with a different subset

Furthermore the number of observations that is drawn for each model has to be specified. Which of the strategies for drawing observations is best, is an open issue and can differ from dataset to dataset. 

In the prediction phase the predictions of the models are aggregated. This can be done in different ways. In classification 
problems the most common class can be predicted (majority vote) or the fraction of the models that voted for each class can be 
regarded. In regression problems it can be aggregated by taking the mean of all models or the median or some other desired 
statistic. Also quantiles can be extracted. By taking out many quantiles of these models, a cumulative distribution function can be estimated and thus also a density. Taking quantiles is a way to get more informations out of the models.

There are several advantages of bagging:




More randomness in random forests are introduced by the random draw of features in each split. This will be topic in one of the 
following posts. 
